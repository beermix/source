diff --color -Naur linux-4.19.150-orig/990-net-tcp_bbr-refactor-bbr_target_cwnd-for-general-inf.patch linux-4.19.150/990-net-tcp_bbr-refactor-bbr_target_cwnd-for-general-inf.patch
--- linux-4.19.150-orig/990-net-tcp_bbr-refactor-bbr_target_cwnd-for-general-inf.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/990-net-tcp_bbr-refactor-bbr_target_cwnd-for-general-inf.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,141 @@
+From 93550e2dc1521b4a126ec86fee72d818ae48a048 Mon Sep 17 00:00:00 2001
+From: Priyaranjan Jha <priyarjha@google.com>
+Date: Thu, 29 Mar 2018 16:14:24 -0700
+Subject: [PATCH 1/6] net-tcp_bbr: refactor bbr_target_cwnd() for general
+ inflight provisioning
+
+Because bbr_target_cwnd() is really a general-purpose BBR helper for
+computing some volume of inflight data as a function of the estimated
+BDP, refactor it into following helper functions:
+- bbr_bdp()
+- bbr_quantization_budget()
+- bbr_inflight()
+
+Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
+Signed-off-by: Neal Cardwell <ncardwell@google.com>
+Signed-off-by: Yuchung Cheng <ycheng@google.com>
+---
+ net/ipv4/tcp_bbr.c | 58 +++++++++++++++++++++++++++++-----------------
+ 1 file changed, 37 insertions(+), 21 deletions(-)
+
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -315,30 +315,19 @@ static void bbr_cwnd_event(struct sock *
+ 	}
+ }
+ 
+-/* Find target cwnd. Right-size the cwnd based on min RTT and the
+- * estimated bottleneck bandwidth:
++/* Calculate bdp based on min RTT and the estimated bottleneck bandwidth:
+  *
+- * cwnd = bw * min_rtt * gain = BDP * gain
++ * bdp = bw * min_rtt * gain
+  *
+  * The key factor, gain, controls the amount of queue. While a small gain
+  * builds a smaller queue, it becomes more vulnerable to noise in RTT
+  * measurements (e.g., delayed ACKs or other ACK compression effects). This
+  * noise may cause BBR to under-estimate the rate.
+- *
+- * To achieve full performance in high-speed paths, we budget enough cwnd to
+- * fit full-sized skbs in-flight on both end hosts to fully utilize the path:
+- *   - one skb in sending host Qdisc,
+- *   - one skb in sending host TSO/GSO engine
+- *   - one skb being received by receiver host LRO/GRO/delayed-ACK engine
+- * Don't worry, at low rates (bbr_min_tso_rate) this won't bloat cwnd because
+- * in such cases tso_segs_goal is 1. The minimum cwnd is 4 packets,
+- * which allows 2 outstanding 2-packet sequences, to try to keep pipe
+- * full even with ACK-every-other-packet delayed ACKs.
+  */
+-static u32 bbr_target_cwnd(struct sock *sk, u32 bw, int gain)
++static u32 bbr_bdp(struct sock *sk, u32 bw, int gain)
+ {
+ 	struct bbr *bbr = inet_csk_ca(sk);
+-	u32 cwnd;
++	u32 bdp;
+ 	u64 w;
+ 
+ 	/* If we've never had a valid RTT sample, cap cwnd at the initial
+@@ -353,8 +342,23 @@ static u32 bbr_target_cwnd(struct sock *
+ 	w = (u64)bw * bbr->min_rtt_us;
+ 
+ 	/* Apply a gain to the given value, then remove the BW_SCALE shift. */
+-	cwnd = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;
++	bdp = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;
++
++	return bdp;
++}
+ 
++/* To achieve full performance in high-speed paths, we budget enough cwnd to
++ * fit full-sized skbs in-flight on both end hosts to fully utilize the path:
++ *   - one skb in sending host Qdisc,
++ *   - one skb in sending host TSO/GSO engine
++ *   - one skb being received by receiver host LRO/GRO/delayed-ACK engine
++ * Don't worry, at low rates (bbr_min_tso_rate) this won't bloat cwnd because
++ * in such cases tso_segs_goal is 1. The minimum cwnd is 4 packets,
++ * which allows 2 outstanding 2-packet sequences, to try to keep pipe
++ * full even with ACK-every-other-packet delayed ACKs.
++ */
++static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd, int gain)
++{
+ 	/* Allow enough full-sized skbs in flight to utilize end systems. */
+ 	cwnd += 3 * bbr_tso_segs_goal(sk);
+ 
+@@ -368,6 +372,17 @@ static u32 bbr_target_cwnd(struct sock *
+ 	return cwnd;
+ }
+ 
++/* Find inflight based on min RTT and the estimated bottleneck bandwidth. */
++static u32 bbr_inflight(struct sock *sk, u32 bw, int gain)
++{
++	u32 inflight;
++
++	inflight = bbr_bdp(sk, bw, gain);
++	inflight = bbr_quantization_budget(sk, inflight, gain);
++
++	return inflight;
++}
++
+ /* An optimization in BBR to reduce losses: On the first round of recovery, we
+  * follow the packet conservation principle: send P packets per P packets acked.
+  * After that, we slow-start and send at most 2*P packets per P packets acked.
+@@ -429,7 +444,8 @@ static void bbr_set_cwnd(struct sock *sk
+ 		goto done;
+ 
+ 	/* If we're below target cwnd, slow start cwnd toward target cwnd. */
+-	target_cwnd = bbr_target_cwnd(sk, bw, gain);
++	target_cwnd = bbr_bdp(sk, bw, gain);
++	target_cwnd = bbr_quantization_budget(sk, target_cwnd, gain);
+ 	if (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */
+ 		cwnd = min(cwnd + acked, target_cwnd);
+ 	else if (cwnd < target_cwnd || tp->delivered < TCP_INIT_CWND)
+@@ -470,14 +486,14 @@ static bool bbr_is_next_cycle_phase(stru
+ 	if (bbr->pacing_gain > BBR_UNIT)
+ 		return is_full_length &&
+ 			(rs->losses ||  /* perhaps pacing_gain*BDP won't fit */
+-			 inflight >= bbr_target_cwnd(sk, bw, bbr->pacing_gain));
++			 inflight >= bbr_inflight(sk, bw, bbr->pacing_gain));
+ 
+ 	/* A pacing_gain < 1.0 tries to drain extra queue we added if bw
+ 	 * probing didn't find more bw. If inflight falls to match BDP then we
+ 	 * estimate queue is drained; persisting would underutilize the pipe.
+ 	 */
+ 	return is_full_length ||
+-		inflight <= bbr_target_cwnd(sk, bw, BBR_UNIT);
++		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
+ }
+ 
+ static void bbr_advance_cycle_phase(struct sock *sk)
+@@ -737,11 +753,11 @@ static void bbr_check_drain(struct sock
+ 		bbr->pacing_gain = bbr_drain_gain;	/* pace slow to drain */
+ 		bbr->cwnd_gain = bbr_high_gain;	/* maintain cwnd */
+ 		tcp_sk(sk)->snd_ssthresh =
+-				bbr_target_cwnd(sk, bbr_max_bw(sk), BBR_UNIT);
++				bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
+ 	}	/* fall through to check if in-flight is already small: */
+ 	if (bbr->mode == BBR_DRAIN &&
+ 	    tcp_packets_in_flight(tcp_sk(sk)) <=
+-	    bbr_target_cwnd(sk, bbr_max_bw(sk), BBR_UNIT))
++	    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT))
+ 		bbr_reset_probe_bw_mode(sk);  /* we estimate queue is drained */
+ }
+ 
diff --color -Naur linux-4.19.150-orig/991-net-tcp_bbr-adapt-cwnd-based-on-ack-aggregation-esti.patch linux-4.19.150/991-net-tcp_bbr-adapt-cwnd-based-on-ack-aggregation-esti.patch
--- linux-4.19.150-orig/991-net-tcp_bbr-adapt-cwnd-based-on-ack-aggregation-esti.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/991-net-tcp_bbr-adapt-cwnd-based-on-ack-aggregation-esti.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,254 @@
+From dd5e4b290c158bed19bcd40f32ff5b18f5a96070 Mon Sep 17 00:00:00 2001
+From: Priyaranjan Jha <priyarjha@google.com>
+Date: Thu, 29 Mar 2018 16:45:03 -0700
+Subject: [PATCH 2/6] net-tcp_bbr: adapt cwnd based on ack aggregation
+ estimation
+
+Aggregation effects are extremely common with wifi, cellular, and cable
+modem link technologies, ACK decimation in middleboxes, and LRO and GRO
+in receiving hosts. The aggregation can happen in either direction,
+data or ACKs, but in either case the aggregation effect is visible
+to the sender in the ACK stream.
+
+Previously BBR's sending was often limited by cwnd under severe ACK
+aggregation/decimation because BBR sized the cwnd at 2*BDP. If packets
+were acked in bursts after long delays (e.g. one ACK acking 5*BDP after
+5*RTT), BBR's sending was halted after sending 2*BDP over 2*RTT, leaving
+the bottleneck idle for potentially long periods. Note that loss-based
+congestion control does not have this issue because when facing
+aggregation it continues increasing cwnd after bursts of ACKs, growing
+cwnd until the buffer is full.
+
+To achieve good throughput in the presence of aggregation effects, this
+algorithm allows the BBR sender to put extra data in flight to keep the
+bottleneck utilized during silences in the ACK stream that it has evidence
+to suggest were caused by aggregation.
+
+A summary of the algorithm: when a burst of packets are acked by a
+stretched ACK or a burst of ACKs or both, BBR first estimates the expected
+amount of data that should have been acked, based on its estimated
+bandwidth. Then the surplus ("extra_acked") is recorded in a windowed-max
+filter to estimate the recent level of observed ACK aggregation. The window
+of the filter is empirically set to 10 round trips, roughly matching the
+bandwidth filter window. Then cwnd is increased by the ACK aggregation
+estimate. The larger cwnd avoids BBR being cwnd-limited in the face of
+ACK silences that recent history suggests were caused by aggregation.
+As a sanity check, the ACK aggregation degree is upper-bounded by the
+cwnd (at the time of measurement) and a global max of BW * 100ms.
+
+In our internal testing, we observed a significant increase in BBR
+throughput, in a basic wifi setup.
+- Host1 (ethernet) -> AP -> Host2 (wifi)
+- 4x increase in 2.4 GHz (~25 Mbps to ~100 Mbps)
+- 10x increase in 5.0 GHz (~25 Mbps to ~260 Mbps)
+
+This is based on Ian Swett's max_ack_height_ algorithm from the
+QUIC BBR implementation.
+
+Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
+Signed-off-by: Neal Cardwell <ncardwell@google.com>
+Reviewed-by: Yuchung Cheng <ycheng@google.com>
+---
+ include/net/inet_connection_sock.h |   4 +-
+ net/ipv4/tcp_bbr.c                 | 120 +++++++++++++++++++++++++++++
+ 2 files changed, 122 insertions(+), 2 deletions(-)
+
+--- a/include/net/inet_connection_sock.h
++++ b/include/net/inet_connection_sock.h
+@@ -139,8 +139,8 @@ struct inet_connection_sock {
+ 	} icsk_mtup;
+ 	u32			  icsk_user_timeout;
+ 
+-	u64			  icsk_ca_priv[88 / sizeof(u64)];
+-#define ICSK_CA_PRIV_SIZE      (11 * sizeof(u64))
++	u64			  icsk_ca_priv[104 / sizeof(u64)];
++#define ICSK_CA_PRIV_SIZE      (13 * sizeof(u64))
+ };
+ 
+ #define ICSK_TIME_RETRANS	1	/* Retransmit timer */
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -115,6 +115,14 @@ struct bbr {
+ 		unused_b:5;
+ 	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */
+ 	u32	full_bw;	/* recent bw, to estimate if pipe is full */
++
++	/* For tracking ACK aggregation: */
++	u64	ack_epoch_mstamp;	/* start of ACK sampling epoch */
++	u16	extra_acked[2];		/* max excess data ACKed in epoch */
++	u32	ack_epoch_acked:20,	/* packets (S)ACKed in sampling epoch */
++		extra_acked_win_rtts:5,	/* age of extra_acked, in round trips */
++		extra_acked_win_idx:1,	/* current index in extra_acked array */
++		unused1:6;
+ };
+ 
+ #define CYCLE_LEN	8	/* number of phases in a pacing gain cycle */
+@@ -176,6 +184,15 @@ static const u32 bbr_lt_bw_max_rtts = 48
+ 
+ static void bbr_check_probe_rtt_done(struct sock *sk);
+ 
++/* Gain factor for adding extra_acked to target cwnd: */
++static const int bbr_extra_acked_gain = BBR_UNIT;
++/* Window length of extra_acked window. Max allowed val is 31. */
++static const u32 bbr_extra_acked_win_rtts = 10;
++/* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
++static const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
++/* Time period for clamping cwnd increment due to ack aggregation */
++static const u32 bbr_extra_acked_max_us = 100 * 1000;
++
+ /* Do we estimate that STARTUP filled the pipe? */
+ static bool bbr_full_bw_reached(const struct sock *sk)
+ {
+@@ -200,6 +217,16 @@ static u32 bbr_bw(const struct sock *sk)
+ 	return bbr->lt_use_bw ? bbr->lt_bw : bbr_max_bw(sk);
+ }
+ 
++/* Return maximum extra acked in past k-2k round trips,
++ * where k = bbr_extra_acked_win_rtts.
++ */
++static u16 bbr_extra_acked(const struct sock *sk)
++{
++	struct bbr *bbr = inet_csk_ca(sk);
++
++	return max(bbr->extra_acked[0], bbr->extra_acked[1]);
++}
++
+ /* Return rate in bytes per second, optionally with a gain.
+  * The order here is chosen carefully to avoid overflow of u64. This should
+  * work for input rates of up to 2.9Tbit/sec and gain of 2.89x.
+@@ -305,6 +332,8 @@ static void bbr_cwnd_event(struct sock *
+ 
+ 	if (event == CA_EVENT_TX_START && tp->app_limited) {
+ 		bbr->idle_restart = 1;
++		bbr->ack_epoch_mstamp = tp->tcp_mstamp;
++		bbr->ack_epoch_acked = 0;
+ 		/* Avoid pointless buffer overflows: pace at est. bw if we don't
+ 		 * need more speed (we're restarting from idle and app-limited).
+ 		 */
+@@ -383,6 +412,22 @@ static u32 bbr_inflight(struct sock *sk,
+ 	return inflight;
+ }
+ 
++/* Find the cwnd increment based on estimate of ack aggregation */
++static u32 bbr_ack_aggregation_cwnd(struct sock *sk)
++{
++	u32 max_aggr_cwnd, aggr_cwnd = 0;
++
++	if (bbr_extra_acked_gain && bbr_full_bw_reached(sk)) {
++		max_aggr_cwnd = ((u64)bbr_bw(sk) * bbr_extra_acked_max_us)
++				/ BW_UNIT;
++		aggr_cwnd = (bbr_extra_acked_gain * bbr_extra_acked(sk))
++			     >> BBR_SCALE;
++		aggr_cwnd = min(aggr_cwnd, max_aggr_cwnd);
++	}
++
++	return aggr_cwnd;
++}
++
+ /* An optimization in BBR to reduce losses: On the first round of recovery, we
+  * follow the packet conservation principle: send P packets per P packets acked.
+  * After that, we slow-start and send at most 2*P packets per P packets acked.
+@@ -445,7 +490,13 @@ static void bbr_set_cwnd(struct sock *sk
+ 
+ 	/* If we're below target cwnd, slow start cwnd toward target cwnd. */
+ 	target_cwnd = bbr_bdp(sk, bw, gain);
++
++	/* Increment the cwnd to account for excess ACKed data that seems
++	 * due to aggregation (of data and/or ACKs) visible in the ACK stream.
++	 */
++	target_cwnd += bbr_ack_aggregation_cwnd(sk);
+ 	target_cwnd = bbr_quantization_budget(sk, target_cwnd, gain);
++
+ 	if (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */
+ 		cwnd = min(cwnd + acked, target_cwnd);
+ 	else if (cwnd < target_cwnd || tp->delivered < TCP_INIT_CWND)
+@@ -716,6 +767,67 @@ static void bbr_update_bw(struct sock *s
+ 	}
+ }
+ 
++/* Estimates the windowed max degree of ack aggregation.
++ * This is used to provision extra in-flight data to keep sending during
++ * inter-ACK silences.
++ *
++ * Degree of ack aggregation is estimated as extra data acked beyond expected.
++ *
++ * max_extra_acked = "maximum recent excess data ACKed beyond max_bw * interval"
++ * cwnd += max_extra_acked
++ *
++ * Max extra_acked is clamped by cwnd and bw * bbr_extra_acked_max_us (100 ms).
++ * Max filter is an approximate sliding window of 10-20 (packet timed) round
++ * trips.
++ */
++static void bbr_update_ack_aggregation(struct sock *sk,
++				       const struct rate_sample *rs)
++{
++	u32 epoch_us, expected_acked, extra_acked;
++	struct bbr *bbr = inet_csk_ca(sk);
++	struct tcp_sock *tp = tcp_sk(sk);
++
++	if (!bbr_extra_acked_gain || rs->acked_sacked <= 0 ||
++	    rs->delivered < 0 || rs->interval_us <= 0)
++		return;
++
++	if (bbr->round_start) {
++		bbr->extra_acked_win_rtts = min(0x1F,
++						bbr->extra_acked_win_rtts + 1);
++		if (bbr->extra_acked_win_rtts >= bbr_extra_acked_win_rtts) {
++			bbr->extra_acked_win_rtts = 0;
++			bbr->extra_acked_win_idx = bbr->extra_acked_win_idx ?
++						   0 : 1;
++			bbr->extra_acked[bbr->extra_acked_win_idx] = 0;
++		}
++	}
++
++	/* Compute how many packets we expected to be delivered over epoch. */
++	epoch_us = tcp_stamp_us_delta(tp->delivered_mstamp,
++				      bbr->ack_epoch_mstamp);
++	expected_acked = ((u64)bbr_bw(sk) * epoch_us) / BW_UNIT;
++
++	/* Reset the aggregation epoch if ACK rate is below expected rate or
++	 * significantly large no. of ack received since epoch (potentially
++	 * quite old epoch).
++	 */
++	if (bbr->ack_epoch_acked <= expected_acked ||
++	    (bbr->ack_epoch_acked + rs->acked_sacked >=
++	     bbr_ack_epoch_acked_reset_thresh)) {
++		bbr->ack_epoch_acked = 0;
++		bbr->ack_epoch_mstamp = tp->delivered_mstamp;
++		expected_acked = 0;
++	}
++
++	/* Compute excess data delivered, beyond what was expected. */
++	bbr->ack_epoch_acked = min(0xFFFFFU,
++				   bbr->ack_epoch_acked + rs->acked_sacked);
++	extra_acked = bbr->ack_epoch_acked - expected_acked;
++	extra_acked = min(extra_acked, tp->snd_cwnd);
++	if (extra_acked > bbr->extra_acked[bbr->extra_acked_win_idx])
++		bbr->extra_acked[bbr->extra_acked_win_idx] = extra_acked;
++}
++
+ /* Estimate when the pipe is full, using the change in delivery rate: BBR
+  * estimates that STARTUP filled the pipe if the estimated bw hasn't changed by
+  * at least bbr_full_bw_thresh (25%) after bbr_full_bw_cnt (3) non-app-limited
+@@ -845,6 +957,7 @@ static void bbr_update_min_rtt(struct so
+ static void bbr_update_model(struct sock *sk, const struct rate_sample *rs)
+ {
+ 	bbr_update_bw(sk, rs);
++	bbr_update_ack_aggregation(sk, rs);
+ 	bbr_update_cycle_phase(sk, rs);
+ 	bbr_check_full_bw_reached(sk, rs);
+ 	bbr_check_drain(sk, rs);
+@@ -895,6 +1008,13 @@ static void bbr_init(struct sock *sk)
+ 	bbr_reset_lt_bw_sampling(sk);
+ 	bbr_reset_startup_mode(sk);
+ 
++	bbr->ack_epoch_mstamp = tp->tcp_mstamp;
++	bbr->ack_epoch_acked = 0;
++	bbr->extra_acked_win_rtts = 0;
++	bbr->extra_acked_win_idx = 0;
++	bbr->extra_acked[0] = 0;
++	bbr->extra_acked[1] = 0;
++
+ 	cmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);
+ }
+ 
diff --color -Naur linux-4.19.150-orig/992-net-tcp_bbr-add-enum-names-for-meaningful-indices-in.patch linux-4.19.150/992-net-tcp_bbr-add-enum-names-for-meaningful-indices-in.patch
--- linux-4.19.150-orig/992-net-tcp_bbr-add-enum-names-for-meaningful-indices-in.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/992-net-tcp_bbr-add-enum-names-for-meaningful-indices-in.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,31 @@
+From 84e87688372ac6ef0b5d1259ab78fc3e7e2ed8c9 Mon Sep 17 00:00:00 2001
+From: Neal Cardwell <ncardwell@google.com>
+Date: Thu, 29 Mar 2018 17:26:59 -0700
+Subject: [PATCH 3/6] net-tcp_bbr: add enum names for meaningful indices into
+ pacing_gain[] array
+
+This commit adds human-readable names for meaningful indices into the
+pacing_gain[] array.
+
+Signed-off-by: Neal Cardwell <ncardwell@google.com>
+Reviewed-by: Yuchung Cheng <ycheng@google.com>
+Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
+Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
+---
+ net/ipv4/tcp_bbr.c | 5 +++++
+ 1 file changed, 5 insertions(+)
+
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -149,6 +149,11 @@ static const int bbr_drain_gain = BBR_UN
+ /* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs: */
+ static const int bbr_cwnd_gain  = BBR_UNIT * 2;
+ /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
++enum bbr_pacing_gain_phase {
++	BBR_BW_PROBE_UP		= 0,
++	BBR_BW_PROBE_DOWN	= 1,
++	BBR_BW_PROBE_CRUISE	= 2,
++};
+ static const int bbr_pacing_gain[] = {
+ 	BBR_UNIT * 5 / 4,	/* probe for more available bw */
+ 	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
diff --color -Naur linux-4.19.150-orig/993-net-tcp-make-tcp_snd_wnd_test-helper-function-as-non.patch linux-4.19.150/993-net-tcp-make-tcp_snd_wnd_test-helper-function-as-non.patch
--- linux-4.19.150-orig/993-net-tcp-make-tcp_snd_wnd_test-helper-function-as-non.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/993-net-tcp-make-tcp_snd_wnd_test-helper-function-as-non.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,50 @@
+From 0dbdf1b57d9f2d76102c0666718a18ee088c5143 Mon Sep 17 00:00:00 2001
+From: Priyaranjan Jha <priyarjha@google.com>
+Date: Fri, 30 Mar 2018 11:28:42 -0700
+Subject: [PATCH 4/6] net-tcp: make tcp_snd_wnd_test() helper function as
+ non-static
+
+This patch makes tcp_snd_wnd_test() helper function as non-static,
+so that it can be used by other modules.
+
+Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
+Reviewed-by: Neal Cardwell <ncardwell@google.com>
+Reviewed-by: Yuchung Cheng <ycheng@google.com>
+---
+ include/net/tcp.h     | 2 ++
+ net/ipv4/tcp_output.c | 6 +++---
+ 2 files changed, 5 insertions(+), 3 deletions(-)
+
+--- a/include/net/tcp.h
++++ b/include/net/tcp.h
+@@ -545,6 +545,8 @@ __u32 cookie_v6_init_sequence(const stru
+ #endif
+ /* tcp_output.c */
+ 
++bool tcp_snd_wnd_test(const struct tcp_sock *tp, const struct sk_buff *skb,
++		      unsigned int cur_mss);
+ void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
+ 			       int nonagle);
+ int __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
+--- a/net/ipv4/tcp_output.c
++++ b/net/ipv4/tcp_output.c
+@@ -1840,9 +1840,8 @@ static inline bool tcp_nagle_test(const
+ }
+ 
+ /* Does at least the first segment of SKB fit into the send window? */
+-static bool tcp_snd_wnd_test(const struct tcp_sock *tp,
+-			     const struct sk_buff *skb,
+-			     unsigned int cur_mss)
++bool tcp_snd_wnd_test(const struct tcp_sock *tp, const struct sk_buff *skb,
++		      unsigned int cur_mss)
+ {
+ 	u32 end_seq = TCP_SKB_CB(skb)->end_seq;
+ 
+@@ -1851,6 +1850,7 @@ static bool tcp_snd_wnd_test(const struc
+ 
+ 	return !after(end_seq, tcp_wnd_end(tp));
+ }
++EXPORT_SYMBOL(tcp_snd_wnd_test);
+ 
+ /* Trim TSO SKB to LEN bytes, put the remaining data into a new packet
+  * which is put after SKB on the list.  It is very much like
diff --color -Naur linux-4.19.150-orig/994-net-tcp_bbr-drain_to_target-drain-inflight-to-BDP-on.patch linux-4.19.150/994-net-tcp_bbr-drain_to_target-drain-inflight-to-BDP-on.patch
--- linux-4.19.150-orig/994-net-tcp_bbr-drain_to_target-drain-inflight-to-BDP-on.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/994-net-tcp_bbr-drain_to_target-drain-inflight-to-BDP-on.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,162 @@
+From 65abee380a9ced613440a468df828b899cea88b3 Mon Sep 17 00:00:00 2001
+From: Neal Cardwell <ncardwell@google.com>
+Date: Fri, 30 Mar 2018 10:30:43 -0700
+Subject: [PATCH 5/6] net-tcp_bbr: drain_to_target: drain inflight to BDP once
+ per cycle
+
+In BBR v1.0 the "drain" phase of the pacing gain cycle holds the
+pacing_gain to 0.75 for essentially 1*min_rtt (or less if inflight
+falls below the BDP).
+
+This commit modifies the behavior of this "drain" phase to attempt to
+"drain to target", adaptively holding this "drain" phase until
+inflight reaches the target level that matches the estimated BDP
+(bandwidth-delay product).
+
+This can significantly reduce the amount of data queued at the
+bottleneck, and hence reduce queuing delay and packet loss, in cases
+where there are multiple flows sharing a bottleneck.
+
+Ingredients to make this work:
+
++ Bounded time in drain phase: We need to refresh our bandwidth
+estimate periodically, before "good" bandwidth samples age out of the
+fillter. So this mechanism makes sure to restart the cycle with the
+bandwidth-probing phase of the cycle at least once per CYCLE_LEN=8 *
+min_rtt.
+
++ Randomized phases: We need to avoid elephants and mice probing and
+draining in sync. Otherwise any mice that are probing in sync with an
+elephant will repeatedly be out-competed for bandwidth, and its
+estimated bandwidth will be driven progressively lower. In this
+commit, each cycle is now of random length: 2 to 8 x min_rtt.
+
++ Aggregation estimator: To adaptively allow higher cwnd with
+aggregation effects, this mechanism needs to be deployed with a
+companion mechanism that can allow higher cwnds when this is required
+due to aggregation on the path.
+
+Signed-off-by: Neal Cardwell <ncardwell@google.com>
+Reviewed-by: Yuchung Cheng <ycheng@google.com>
+Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
+---
+ net/ipv4/tcp_bbr.c | 78 +++++++++++++++++++++++++++++++++++++++++++++-
+ 1 file changed, 77 insertions(+), 1 deletion(-)
+
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -96,9 +96,10 @@ struct bbr {
+ 		prev_ca_state:3,     /* CA state on previous ACK */
+ 		packet_conservation:1,  /* use packet conservation? */
+ 		round_start:1,	     /* start of packet-timed tx->ack round? */
++		cycle_len:4,	     /* phases in this PROBE_BW gain cycle */
+ 		idle_restart:1,	     /* restarting after idle? */
+ 		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
+-		unused:13,
++		unused:9,
+ 		lt_is_sampling:1,    /* taking long-term ("LT") samples now? */
+ 		lt_rtt_cnt:7,	     /* round trips in long-term interval */
+ 		lt_use_bw:1;	     /* use lt_bw as our bw estimate? */
+@@ -198,6 +199,9 @@ static const u32 bbr_ack_epoch_acked_res
+ /* Time period for clamping cwnd increment due to ack aggregation */
+ static const u32 bbr_extra_acked_max_us = 100 * 1000;
+ 
++/* Each cycle, try to hold sub-unity gain until inflight <= BDP. */
++static const bool bbr_drain_to_target = true;	/* default: enabled */
++
+ /* Do we estimate that STARTUP filled the pipe? */
+ static bool bbr_full_bw_reached(const struct sock *sk)
+ {
+@@ -552,6 +556,72 @@ static bool bbr_is_next_cycle_phase(stru
+ 		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
+ }
+ 
++static void bbr_set_cycle_idx(struct sock *sk, int cycle_idx)
++{
++	struct bbr *bbr = inet_csk_ca(sk);
++
++	bbr->cycle_idx = cycle_idx;
++	bbr->pacing_gain = bbr->lt_use_bw ?
++			   BBR_UNIT : bbr_pacing_gain[bbr->cycle_idx];
++}
++
++static void bbr_drain_to_target_cycling(struct sock *sk,
++					const struct rate_sample *rs)
++{
++	struct tcp_sock *tp = tcp_sk(sk);
++	struct bbr *bbr = inet_csk_ca(sk);
++	u32 elapsed_us =
++		tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp);
++	u32 inflight, bw;
++
++	if (bbr->mode != BBR_PROBE_BW)
++		return;
++
++	/* Always need to probe for bw before we forget good bw estimate. */
++	if (elapsed_us > bbr->cycle_len * bbr->min_rtt_us) {
++		/* Start a new PROBE_BW probing cycle of [2 to 8] x min_rtt. */
++		bbr->cycle_mstamp = tp->delivered_mstamp;
++		bbr->cycle_len = CYCLE_LEN - prandom_u32_max(bbr_cycle_rand);
++		bbr_set_cycle_idx(sk, BBR_BW_PROBE_UP);  /* probe bandwidth */
++		return;
++	}
++
++	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
++	 * use the pipe without increasing the queue.
++	 */
++	if (bbr->pacing_gain == BBR_UNIT)
++		return;
++
++	inflight = rs->prior_in_flight;  /* what was in-flight before ACK? */
++	bw = bbr_max_bw(sk);
++
++	/* A pacing_gain < 1.0 tries to drain extra queue we added if bw
++	 * probing didn't find more bw. If inflight falls to match BDP then we
++	 * estimate queue is drained; persisting would underutilize the pipe.
++	 */
++	if (bbr->pacing_gain < BBR_UNIT) {
++		if (inflight <= bbr_inflight(sk, bw, BBR_UNIT))
++			bbr_set_cycle_idx(sk, BBR_BW_PROBE_CRUISE); /* cruise */
++		return;
++	}
++
++	/* A pacing_gain > 1.0 probes for bw by trying to raise inflight to at
++	 * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is
++	 * small (e.g. on a LAN). We do not persist if packets are lost, since
++	 * a path with small buffers may not hold that much. Similarly we exit
++	 * if we were prevented by app/recv-win from reaching the target.
++	 */
++	if (elapsed_us > bbr->min_rtt_us &&
++	    (inflight >= bbr_inflight(sk, bw, bbr->pacing_gain) ||
++	     rs->losses ||         /* perhaps pacing_gain*BDP won't fit */
++	     rs->is_app_limited || /* previously app-limited */
++	     !tcp_send_head(sk) || /* currently app/rwin-limited */
++	     !tcp_snd_wnd_test(tp, tcp_send_head(sk), tp->mss_cache))) {
++		bbr_set_cycle_idx(sk, BBR_BW_PROBE_DOWN);  /* drain queue */
++		return;
++	}
++}
++
+ static void bbr_advance_cycle_phase(struct sock *sk)
+ {
+ 	struct tcp_sock *tp = tcp_sk(sk);
+@@ -569,6 +639,11 @@ static void bbr_update_cycle_phase(struc
+ {
+ 	struct bbr *bbr = inet_csk_ca(sk);
+ 
++	if (bbr_drain_to_target) {
++		bbr_drain_to_target_cycling(sk, rs);
++		return;
++	}
++
+ 	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
+ 		bbr_advance_cycle_phase(sk);
+ }
+@@ -1010,6 +1085,7 @@ static void bbr_init(struct sock *sk)
+ 	bbr->full_bw_cnt = 0;
+ 	bbr->cycle_mstamp = 0;
+ 	bbr->cycle_idx = 0;
++	bbr->cycle_len = 0;
+ 	bbr_reset_lt_bw_sampling(sk);
+ 	bbr_reset_startup_mode(sk);
+ 
diff --color -Naur linux-4.19.150-orig/995-net-tcp_bbr-knobs-for-ack_aggr-and-drain_to_target.patch linux-4.19.150/995-net-tcp_bbr-knobs-for-ack_aggr-and-drain_to_target.patch
--- linux-4.19.150-orig/995-net-tcp_bbr-knobs-for-ack_aggr-and-drain_to_target.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/995-net-tcp_bbr-knobs-for-ack_aggr-and-drain_to_target.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,37 @@
+From e37c05cf988c4f18d4b10a93a4680eb24dbf92b0 Mon Sep 17 00:00:00 2001
+From: Priyaranjan Jha <priyarjha@google.com>
+Date: Mon, 2 Apr 2018 19:48:05 -0400
+Subject: [PATCH 6/6] net-tcp_bbr: knobs for ack_aggr and drain_to_target
+
+This patch adds knobs for ack_aggr and drain_to_target. This is for
+development testing only, and not intended to be in the final version.
+
+Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
+Reviewed-by: Neal Cardwell <ncardwell@google.com>
+---
+ net/ipv4/tcp_bbr.c | 7 +++++--
+ 1 file changed, 5 insertions(+), 2 deletions(-)
+
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -191,7 +191,7 @@ static const u32 bbr_lt_bw_max_rtts = 48
+ static void bbr_check_probe_rtt_done(struct sock *sk);
+ 
+ /* Gain factor for adding extra_acked to target cwnd: */
+-static const int bbr_extra_acked_gain = BBR_UNIT;
++static int bbr_extra_acked_gain = BBR_UNIT;
+ /* Window length of extra_acked window. Max allowed val is 31. */
+ static const u32 bbr_extra_acked_win_rtts = 10;
+ /* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
+@@ -200,7 +200,10 @@ static const u32 bbr_ack_epoch_acked_res
+ static const u32 bbr_extra_acked_max_us = 100 * 1000;
+ 
+ /* Each cycle, try to hold sub-unity gain until inflight <= BDP. */
+-static const bool bbr_drain_to_target = true;	/* default: enabled */
++static bool bbr_drain_to_target = true;	/* default: enabled */
++
++module_param_named(extra_acked_gain,  bbr_extra_acked_gain,  int,    0664);
++module_param_named(drain_to_target,   bbr_drain_to_target,   bool,   0664);
+ 
+ /* Do we estimate that STARTUP filled the pipe? */
+ static bool bbr_full_bw_reached(const struct sock *sk)
diff --color -Naur linux-4.19.150-orig/996-net-tcp_bbr-fix-991.patch linux-4.19.150/996-net-tcp_bbr-fix-991.patch
--- linux-4.19.150-orig/996-net-tcp_bbr-fix-991.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/996-net-tcp_bbr-fix-991.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,38 @@
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -398,7 +398,7 @@ static u32 bbr_bdp(struct sock *sk, u32
+  * which allows 2 outstanding 2-packet sequences, to try to keep pipe
+  * full even with ACK-every-other-packet delayed ACKs.
+  */
+-static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd, int gain)
++static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)
+ {
+ 	/* Allow enough full-sized skbs in flight to utilize end systems. */
+ 	cwnd += 3 * bbr_tso_segs_goal(sk);
+@@ -407,7 +407,7 @@ static u32 bbr_quantization_budget(struc
+ 	cwnd = (cwnd + 1) & ~1U;
+ 
+ 	/* Ensure gain cycling gets inflight above BDP even for small BDPs. */
+-	if (bbr->mode == BBR_PROBE_BW && gain > BBR_UNIT)
++	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == 0)
+ 		cwnd += 2;
+ 
+ 	return cwnd;
+@@ -419,7 +419,7 @@ static u32 bbr_inflight(struct sock *sk,
+ 	u32 inflight;
+ 
+ 	inflight = bbr_bdp(sk, bw, gain);
+-	inflight = bbr_quantization_budget(sk, inflight, gain);
++	inflight = bbr_quantization_budget(sk, inflight);
+ 
+ 	return inflight;
+ }
+@@ -507,7 +507,7 @@ static void bbr_set_cwnd(struct sock *sk
+ 	 * due to aggregation (of data and/or ACKs) visible in the ACK stream.
+ 	 */
+ 	target_cwnd += bbr_ack_aggregation_cwnd(sk);
+-	target_cwnd = bbr_quantization_budget(sk, target_cwnd, gain);
++	target_cwnd = bbr_quantization_budget(sk, target_cwnd);
+ 
+ 	if (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */
+ 		cwnd = min(cwnd + acked, target_cwnd);
diff --color -Naur linux-4.19.150-orig/997-fix-bbr-compile.patch linux-4.19.150/997-fix-bbr-compile.patch
--- linux-4.19.150-orig/997-fix-bbr-compile.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/997-fix-bbr-compile.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,11 @@
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -400,6 +400,8 @@ static u32 bbr_bdp(struct sock *sk, u32
+  */
+ static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)
+ {
++	struct bbr *bbr = inet_csk_ca(sk);
++
+ 	/* Allow enough full-sized skbs in flight to utilize end systems. */
+ 	cwnd += 3 * bbr_tso_segs_goal(sk);
+ 
diff --color -Naur linux-4.19.150-orig/998-bbr-fix-rate.patch linux-4.19.150/998-bbr-fix-rate.patch
--- linux-4.19.150-orig/998-bbr-fix-rate.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/998-bbr-fix-rate.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,51 @@
+From e5fe52d14784df8e08067b2d6a2fa99e01e81a44 Mon Sep 17 00:00:00 2001
+From: Neal Cardwell <ncardwell@google.com>
+Date: Tue, 11 Jun 2019 12:26:55 -0400
+Subject: [PATCH] net-tcp_bbr: broaden app-limited rate sample detection
+
+This commit is a bug fix for the Linux TCP app-limited
+(application-limited) logic that is used for collecting rate
+(bandwidth) samples.
+
+Previously the app-limited logic only looked for "bubbles" of
+silence in between application writes, by checking at the start
+of each sendmsg. But "bubbles" of silence can also happen before
+retransmits: e.g. bubbles can happen between an application write
+and a retransmit, or between two retransmits.
+
+Retransmits are triggered by ACKs or timers. So this commit checks
+for bubbles of app-limited silence upon ACKs or timers.
+
+Why does this commit check for app-limited state at the start of
+ACKs and timer handling? Because at that point we know whether
+inflight was fully using the cwnd.  During processing the ACK or
+timer event we often change the cwnd; after changing the cwnd we
+can't know whether inflight was fully using the old cwnd.
+
+Origin-9xx-SHA1: 3fe9b53291e018407780fb8c356adb5666722cbc
+Change-Id: I37221506f5166877c2b110753d39bb0757985e68
+---
+ net/ipv4/tcp_input.c | 1 +
+ net/ipv4/tcp_timer.c | 1 +
+ 2 files changed, 2 insertions(+)
+
+--- a/net/ipv4/tcp_input.c
++++ b/net/ipv4/tcp_input.c
+@@ -3637,6 +3637,7 @@ static int tcp_ack(struct sock *sk, cons
+ 
+ 	prior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp->snd_una;
+ 	rs.prior_in_flight = tcp_packets_in_flight(tp);
++	tcp_rate_check_app_limited(sk);
+ 
+ 	/* ts_recent update must be made after we are sure that the packet
+ 	 * is in window.
+--- a/net/ipv4/tcp_timer.c
++++ b/net/ipv4/tcp_timer.c
+@@ -585,6 +585,7 @@ void tcp_write_timer_handler(struct sock
+ 		goto out;
+ 	}
+ 
++	tcp_rate_check_app_limited(sk);
+ 	tcp_mstamp_refresh(tcp_sk(sk));
+ 	event = icsk->icsk_pending;
+ 
diff --color -Naur linux-4.19.150-orig/998-bbr-fix-tcp.patch linux-4.19.150/998-bbr-fix-tcp.patch
--- linux-4.19.150-orig/998-bbr-fix-tcp.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/998-bbr-fix-tcp.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,122 @@
+From 1cd7f04b17b112f8263b29f5c972edab91c46278 Mon Sep 17 00:00:00 2001
+From: Neal Cardwell <ncardwell@google.com>
+Date: Fri, 27 Sep 2019 17:10:26 -0400
+Subject: [PATCH] net-tcp: re-generalize TSO sizing in TCP CC module API
+
+Reorganize the API for CC modules so that the CC module once again
+gets complete control of the TSO sizing decision. This is how the API
+was set up around 2016 and the initial BBRv1 upstreaming. Later Eric
+Dumazet simplified it. But with wider testing it now seems that to
+avoid CPU regressions BBR needs to have a different TSO sizing
+function.
+
+This is necessary to handle cases where there are many flows
+bottlenecked on the sender host's NIC, in which case BBR's pacing rate
+is much lower than CUBIC/Reno/DCTCP's. Why does this happen? Because
+BBR's pacing rate adapts to the low bandwidth share each flow sees. By
+contrast, CUBIC/Reno/DCTCP see no loss or ECN, so they grow a very
+large cwnd, and thus large pacing rate and large TSO burst size.
+
+Change-Id: Ic8ccfdbe4010ee8d4bf6a6334c48a2fceb2171ea
+---
+ include/net/tcp.h     |  4 ++--
+ net/ipv4/tcp_bbr.c    | 37 ++++++++++++++++++++++++++-----------
+ net/ipv4/tcp_output.c | 11 +++++------
+ 3 files changed, 33 insertions(+), 19 deletions(-)
+
+--- a/include/net/tcp.h
++++ b/include/net/tcp.h
+@@ -1029,8 +1029,8 @@ struct tcp_congestion_ops {
+ 	u32  (*undo_cwnd)(struct sock *sk);
+ 	/* hook for packet ack accounting (optional) */
+ 	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);
+-	/* override sysctl_tcp_min_tso_segs */
+-	u32 (*min_tso_segs)(struct sock *sk);
++	/* pick target number of segments per TSO/GSO skb (optional): */
++	u32 (*tso_segs)(struct sock *sk, unsigned int mss_now);
+ 	/* returns the multiplier used in tcp_sndbuf_expand (optional) */
+ 	u32 (*sndbuf_expand)(struct sock *sk);
+ 	/* call when packets are delivered to update cwnd and pacing rate,
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -304,25 +304,40 @@ static void bbr_set_pacing_rate(struct s
+ 		sk->sk_pacing_rate = rate;
+ }
+ 
+-/* override sysctl_tcp_min_tso_segs */
+ static u32 bbr_min_tso_segs(struct sock *sk)
+ {
+ 	return sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
+ }
+ 
++/* Return the number of segments BBR would like in a TSO/GSO skb, given
++ * a particular max gso size as a constraint.
++ */
++static u32 bbr_tso_segs_generic(struct sock *sk, unsigned int mss_now,
++				u32 gso_max_size)
++{
++	u32 segs;
++	u64 bytes;
++
++	/* Budget a TSO/GSO burst size allowance based on bw (pacing_rate). */
++	bytes = sk->sk_pacing_rate >> sk->sk_pacing_shift;
++
++	bytes = min_t(u32, bytes, gso_max_size - 1 - MAX_TCP_HEADER);
++	segs = max_t(u32, bytes / mss_now, bbr_min_tso_segs(sk));
++	return segs;
++}
++
++/* Custom tcp_tso_autosize() for BBR, used at transmit time to cap skb size. */
++static u32  bbr_tso_segs(struct sock *sk, unsigned int mss_now)
++{
++	return bbr_tso_segs_generic(sk, mss_now, sk->sk_gso_max_size);
++}
++
++/* Like bbr_tso_segs(), using mss_cache, ignoring driver's sk_gso_max_size. */
+ static u32 bbr_tso_segs_goal(struct sock *sk)
+ {
+ 	struct tcp_sock *tp = tcp_sk(sk);
+-	u32 segs, bytes;
+-
+-	/* Sort of tcp_tso_autosize() but ignoring
+-	 * driver provided sk_gso_max_size.
+-	 */
+-	bytes = min_t(u32, sk->sk_pacing_rate >> sk->sk_pacing_shift,
+-		      GSO_MAX_SIZE - 1 - MAX_TCP_HEADER);
+-	segs = max_t(u32, bytes / tp->mss_cache, bbr_min_tso_segs(sk));
+ 
+-	return min(segs, 0x7FU);
++	return  bbr_tso_segs_generic(sk, tp->mss_cache, GSO_MAX_SIZE);
+ }
+ 
+ /* Save "last known good" cwnd so we can restore it after losses or PROBE_RTT */
+@@ -1176,7 +1191,7 @@ static struct tcp_congestion_ops tcp_bbr
+ 	.undo_cwnd	= bbr_undo_cwnd,
+ 	.cwnd_event	= bbr_cwnd_event,
+ 	.ssthresh	= bbr_ssthresh,
+-	.min_tso_segs	= bbr_min_tso_segs,
++	.tso_segs	= bbr_tso_segs,
+ 	.get_info	= bbr_get_info,
+ 	.set_state	= bbr_set_state,
+ };
+--- a/net/ipv4/tcp_output.c
++++ b/net/ipv4/tcp_output.c
+@@ -1731,13 +1731,12 @@ static u32 tcp_tso_autosize(const struct
+ static u32 tcp_tso_segs(struct sock *sk, unsigned int mss_now)
+ {
+ 	const struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;
+-	u32 min_tso, tso_segs;
++	u32 tso_segs;
+ 
+-	min_tso = ca_ops->min_tso_segs ?
+-			ca_ops->min_tso_segs(sk) :
+-			sock_net(sk)->ipv4.sysctl_tcp_min_tso_segs;
+-
+-	tso_segs = tcp_tso_autosize(sk, mss_now, min_tso);
++	tso_segs = ca_ops->tso_segs ?
++		ca_ops->tso_segs(sk, mss_now) :
++		tcp_tso_autosize(sk, mss_now,
++				 sock_net(sk)->ipv4.sysctl_tcp_min_tso_segs);
+ 	return min_t(u32, tso_segs, sk->sk_gso_max_segs);
+ }
+ 
diff --color -Naur linux-4.19.150-orig/999-do_div.patch linux-4.19.150/999-do_div.patch
--- linux-4.19.150-orig/999-do_div.patch	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.19.150/999-do_div.patch	2020-09-14 04:10:19.319136300 +0300
@@ -0,0 +1,11 @@
+--- a/net/ipv4/tcp_bbr.c
++++ b/net/ipv4/tcp_bbr.c
+@@ -322,7 +322,7 @@ static u32 bbr_tso_segs_generic(struct s
+ 	bytes = sk->sk_pacing_rate >> sk->sk_pacing_shift;
+ 
+ 	bytes = min_t(u32, bytes, gso_max_size - 1 - MAX_TCP_HEADER);
+-	segs = max_t(u32, bytes / mss_now, bbr_min_tso_segs(sk));
++	segs = max_t(u32, do_div(bytes, mss_now), bbr_min_tso_segs(sk));
+ 	return segs;
+ }
+ 
